{
	"jobConfig": {
		"name": "glue-refined-zone-bovespa",
		"description": "",
		"role": "arn:aws:iam::092888533129:role/LabRole",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 2,
		"maxCapacity": 2,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "glue-refined-zone-bovespa.py",
		"scriptLocation": "s3://aws-glue-assets-092888533129-us-east-1/scripts/",
		"language": "python-3",
		"spark": false,
		"sparkConfiguration": "standard",
		"jobParameters": [
			{
				"key": "--additional-python-modules",
				"value": "awswrangler",
				"existing": false
			}
		],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-07-09T21:15:10.144Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-092888533129-us-east-1/temporary/",
		"etlAutoScaling": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"serverEncryption": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-092888533129-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null,
		"pythonPath": null,
		"logging": false
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import awswrangler as wr\r\nimport boto3\r\nimport sys\r\nimport logging\r\nfrom typing import Dict, List\r\nfrom pyspark.sql import DataFrame, SparkSession, functions as sf\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom awsglue.job import Job\r\nfrom botocore.exceptions import ClientError\r\n\r\n# Configuração de logging\r\nlogging.basicConfig(level=logging.INFO)\r\nlogger = logging.getLogger(__name__)\r\n\r\nREQUIRED_PARAMS: List[str] = [\r\n    'JOB_NAME',\r\n    'TABLE_NAME',\r\n    'DATABASE_NAME',\r\n    'S3_BUCKET',\r\n    'OBJECT_KEY',\r\n    'S3_OUTPUT_BUCKET',\r\n    'S3_OUTPUT_PREFIX',\r\n    'AWS_REGION'\r\n]\r\n\r\nEXPECTED_COLUMNS: List[str] = [\r\n    \"results_segment\", \"results_asset\", \"results_cod\", \"results_type\",\r\n    \"results_theoricalQty\", \"results_part\", \"results_partAcum\", \"header_date\"\r\n]\r\n\r\ndef validate_params(args: Dict[str, str]) -> None:\r\n    \"\"\"\r\n    Valida se todos os parâmetros obrigatórios foram fornecidos.\r\n    \"\"\"\r\n    missing = [param for param in REQUIRED_PARAMS if param not in args or not args[param]]\r\n    if missing:\r\n        logger.error(f\"Parâmetros obrigatórios ausentes: {missing}\")\r\n        raise ValueError(f\"Parâmetros obrigatórios ausentes: {missing}\")\r\n\r\ndef validate_schema(df: DataFrame, expected_columns: List[str]) -> None:\r\n    \"\"\"\r\n    Valida se o DataFrame possui todas as colunas esperadas.\r\n    \"\"\"\r\n    missing = [col for col in expected_columns if col not in df.columns]\r\n    if missing:\r\n        logger.error(f\"Colunas esperadas ausentes: {missing}\")\r\n        raise ValueError(f\"Colunas esperadas ausentes: {missing}\")\r\n\r\ndef read_data(spark: SparkSession, s3_bucket: str, object_key: str) -> DataFrame:\r\n    \"\"\"\r\n    Lê os dados do S3 no formato Parquet.\r\n    \"\"\"\r\n    logger.info(f\"Lendo dados do bucket S3: {s3_bucket} com prefixo: {object_key}\")\r\n    try:\r\n        df = spark.read.parquet(f\"s3://{s3_bucket}/{object_key}\")\r\n        logger.info(f\"Dados lidos com sucesso. Número de registros: {df.count()}\")\r\n        return df\r\n    except Exception as e:\r\n        logger.error(f\"Erro ao ler dados do S3: {e}\")\r\n        raise\r\n\r\ndef drop_and_log_nulls(df: DataFrame) -> DataFrame:\r\n    \"\"\"\r\n    Remove linhas com valores nulos e loga a quantidade de linhas removidas.\r\n    \"\"\"\r\n    qtd_total = df.count()\r\n    df_clean = df.na.drop()\r\n    qtd_rows_unic = df_clean.count()\r\n    logger.info(f\"Linhas removidas por valores nulos: {qtd_total - qtd_rows_unic}\")\r\n    return df_clean\r\n\r\ndef process_data(df: DataFrame) -> DataFrame:\r\n    \"\"\"\r\n    Realiza o processamento e limpeza dos dados.\r\n    \"\"\"\r\n    logger.info(\"Processando dados...\")\r\n    try:\r\n        validate_schema(df, EXPECTED_COLUMNS)\r\n        logger.info(\"Validação de esquema concluída com sucesso.\")\r\n        df = df.drop(\r\n                \"page_pageNumber\", \"page_pageSize\", \"page_totalRecords\", \"page_totalPages\",\r\n                \"header_text\", \"header_part\", \"header_partAcum\", \"header_textReductor\",\r\n                \"header_reductor\", \"header_theoricalQty\")\r\n        df.cache()\r\n        logger.info(\"Cache aplicado aos dados.\")\r\n        df = drop_and_log_nulls(df)\r\n        df = (\r\n            df.withColumnsRenamed({\r\n                \"results_segment\": \"nom_setor\",\r\n                \"results_asset\": \"nom_empresa\",\r\n                \"results_cod\": \"cod_acao\",\r\n                \"results_type\": \"des_tipo_acao\",\r\n                \"results_theoricalQty\": \"qtd_teorica\",\r\n                \"results_part\": \"perc_participacao_setor\",\r\n                \"results_partAcum\": \"perc_participacao_setor_acumulada\",\r\n                \"header_date\": \"data_ref\"\r\n            })\r\n            .withColumn(\"nom_setor\", sf.trim(\"nom_setor\"))\r\n            .withColumn(\"nom_empresa\", sf.trim(\"nom_empresa\"))\r\n            .withColumn(\"cod_acao\", sf.trim(\"cod_acao\"))\r\n            .withColumn(\"des_tipo_acao\", sf.trim(\"des_tipo_acao\"))\r\n            .withColumn(\"perc_participacao_setor\", sf.regexp_replace(\"perc_participacao_setor\", \",\", \".\").cast(\"decimal(5,3)\"))\r\n            .withColumn(\"perc_participacao_setor_acumulada\", sf.regexp_replace(\"perc_participacao_setor_acumulada\", \",\", \".\").cast(\"decimal(5,3)\"))\r\n            .withColumn(\"qtd_teorica\", sf.regexp_replace(\"qtd_teorica\", \"[. ]\", \"\").cast(\"bigint\"))\r\n            .withColumn(\"data_ref\", sf.to_date(\"data_ref\", \"dd/MM/yy\"))\r\n            .withColumn(\"year\", sf.year(\"data_ref\"))\r\n            .withColumn(\"month\", sf.month(\"data_ref\"))\r\n            .withColumn(\"day\", sf.day(\"data_ref\"))\r\n        )\r\n        logger.info(f\"Processamento de dados concluído. Registros finais: {df.count()}\")\r\n        return df\r\n    except Exception as e:\r\n        logger.error(f\"Erro ao processar dados: {e}\")\r\n        raise\r\n    \r\ndef aggregate_data(df: DataFrame) -> DataFrame:\r\n    \"\"\"\r\n    Realiza a agregação dos dados por setor, empresa e data de referência.\r\n    \"\"\"\r\n    try:\r\n        df = (\r\n            df.groupBy(\"nom_setor\",\"nom_empresa\",\"data_ref\",\"year\",\"month\",\"day\") \\\r\n            .agg(\r\n                sf.count(sf.col(\"cod_acao\")).alias(\"qtd_registros\"),\r\n                sf.count_distinct(sf.col(\"cod_acao\")).alias(\"qtd_acao\"),\r\n                sf.count_distinct(sf.col(\"des_tipo_acao\")).alias(\"qtd_tipos_acao\"),\r\n                sf.sum(sf.col(\"qtd_teorica\")).alias(\"qtd_teorica_acumulada\"),\r\n                sf.max(sf.col(\"qtd_teorica\")).alias(\"qtd_teorica_max\"),\r\n                sf.min(sf.col(\"qtd_teorica\")).alias(\"qtd_teorica_min\"),\r\n                sf.avg(sf.col(\"perc_participacao_setor\")).alias(\"avg_participacao_setor_total\"),\r\n                sf.avg(sf.col(\"perc_participacao_setor_acumulada\")).alias(\"avg_participacao_setor_acumulada_total\")\r\n            ) \\\r\n            .withColumn(\"dth_etl_processamento\", sf.current_timestamp()) \\\r\n            .withColumn(\"qtd_dias_atraso\", sf.date_diff(\"data_ref\",\"dth_etl_processamento\")) \\\r\n            .select(\r\n                \"nom_empresa\",\"qtd_registros\",\"qtd_acao\",\"qtd_tipos_acao\",\"qtd_teorica_acumulada\",\"qtd_teorica_max\",\r\n                \"qtd_teorica_min\",\"avg_participacao_setor_total\",\"avg_participacao_setor_acumulada_total\",\"qtd_dias_atraso\",\r\n                \"data_ref\",\"dth_etl_processamento\",\"year\",\"month\",\"day\",\"nom_setor\"\r\n            )\r\n        )\r\n        logger.info(f\"Agregação de dados concluída. Registros agregados: {df.count()}\")\r\n        return df\r\n    except Exception as e:\r\n        logger.error(f\"Erro ao agregar dados: {e}\")\r\n        raise\r\n\r\ndef write_data(df: DataFrame,s3_output_bucket: str,s3_output_prefix: str,partition_keys: List[str] = [\"year\", \"month\", \"day\"],compression: str = \"snappy\") -> None:\r\n    \"\"\"\r\n    Escreve os dados processados no S3 particionados e cataloga no Glue.\r\n    \"\"\"\r\n    try:\r\n        df.write \\\r\n            .mode(\"overwrite\") \\\r\n            .option(\"compression\", compression) \\\r\n            .option(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\") \\\r\n            .partitionBy(*partition_keys) \\\r\n            .format(\"parquet\") \\\r\n            .parquet(f\"s3://{s3_output_bucket}/{s3_output_prefix}\")\r\n        logger.info(f\"Dados gravados com sucesso em s3://{s3_output_bucket}/{s3_output_prefix}\")\r\n    except Exception as e:\r\n        logger.error(f\"Erro ao gravar dados no S3: {e}\")\r\n        raise\r\n\r\n# Catalogação automática usando boto3\r\ndef create_table_if_not_exists(database_name:str,table_name:str,s3_output_bucket: str,s3_output_prefix: str,aws_region:str) -> None:\r\n    glue = boto3.client('glue', region_name=aws_region)\r\n    try:\r\n        glue.get_table(DatabaseName=database_name, Name=table_name)\r\n        logger.info(f\"Tabela {table_name} já existe no catálogo {database_name}. Nenhuma ação necessária.\")\r\n    except ClientError as e:\r\n        if e.response['Error']['Code'] == 'EntityNotFoundException':\r\n            logger.info(f\"Tabela {table_name} não existe. Criando tabela no catálogo Glue...\")\r\n            glue.create_table(\r\n                DatabaseName=database_name,\r\n                TableInput={\r\n                    'Name': table_name,\r\n                    'StorageDescriptor': {\r\n                        'Columns': [\r\n                            {'Name': 'nom_empresa', 'Type': 'string'},\r\n                            {'Name': 'qtd_registros', 'Type': 'bigint'},\r\n                            {'Name': 'qtd_acao', 'Type': 'bigint'},\r\n                            {'Name': 'qtd_tipos_acao', 'Type': 'bigint'},\r\n                            {'Name': 'qtd_teorica_acumulada', 'Type': 'bigint'},\r\n                            {'Name': 'qtd_teorica_max', 'Type': 'bigint'},\r\n                            {'Name': 'qtd_teorica_min', 'Type': 'bigint'},\r\n                            {'Name': 'qtd_dias_atraso', 'Type': 'int'},\r\n                            {'Name': 'avg_participacao_setor_total', 'Type': 'double'},\r\n                            {'Name': 'avg_participacao_setor_acumulada_total', 'Type': 'double'},\r\n                            {'Name': 'data_ref', 'Type': 'date'},\r\n                            {'Name': 'dth_etl_processamento', 'Type': 'timestamp'},\r\n                        ],\r\n                        'Location': f\"s3://{s3_output_bucket}/{s3_output_prefix}\",\r\n                        'InputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat',\r\n                        'OutputFormat': 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat',\r\n                        'SerdeInfo': {\r\n                            'SerializationLibrary': 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe',\r\n                            'Parameters': {'serialization.format': '1'}\r\n                        },\r\n                        'Compressed': True,\r\n                        'StoredAsSubDirectories': False\r\n                    },\r\n                    'PartitionKeys': [\r\n                            {'Name': 'year', 'Type': 'int'},\r\n                            {'Name': 'month', 'Type': 'int'},\r\n                            {'Name': 'day', 'Type': 'int'},\r\n                            {'Name': 'nom_setor', 'Type': 'string'}\r\n                        ],\r\n                    'TableType': 'EXTERNAL_TABLE',\r\n                    'Parameters': {\r\n                        'classification': 'parquet'\r\n                    }\r\n                }\r\n            )\r\n            logger.info(f\"Tabela {table_name} criada com sucesso no catálogo {database_name}.\")\r\n        else:\r\n            logger.error(f\"Erro ao acessar o catálogo Glue: {e}\")\r\n            raise\r\n\r\ndef msck_repair_table(database_name: str, table_name: str) -> None:\r\n    \"\"\"\r\n    Executa o comando MSCK REPAIR TABLE via Athena usando awswrangler.\r\n    \"\"\"\r\n    try:\r\n        logger.info(f\"Executando MSCK REPAIR TABLE {database_name}.{table_name} via Athena...\")\r\n        wr.athena.start_query_execution(\r\n            sql=f\"MSCK REPAIR TABLE {database_name}.{table_name}\",\r\n            database=database_name,\r\n            workgroup=\"primary\"\r\n        )\r\n        logger.info(\"MSCK REPAIR TABLE executado com sucesso.\")\r\n    except Exception as e:\r\n        logger.error(f\"Erro ao executar MSCK REPAIR TABLE: {e}\")\r\n        raise\r\n\r\ndef main() -> None:\r\n    \"\"\"\r\n    Função principal do Glue Job.\r\n    \"\"\"\r\n    args = getResolvedOptions(sys.argv, REQUIRED_PARAMS)\r\n    validate_params(args)\r\n\r\n    logger.info(f\"Iniciando o job Glue: {args['JOB_NAME']}\")\r\n    logger.info(f\"Argumentos do job Glue: {args}\")\r\n\r\n    with SparkContext() as sc:\r\n        glueContext = GlueContext(sc)\r\n        spark = glueContext.spark_session\r\n        job = Job(glueContext)\r\n        job.init(args['JOB_NAME'], args)\r\n\r\n        df = read_data(spark=spark, s3_bucket=args['S3_BUCKET'], object_key=args['OBJECT_KEY'])\r\n        processed_df = process_data(df=df)\r\n        agg_df = aggregate_data(df=processed_df)\r\n        write_data(\r\n                df= agg_df,\r\n                s3_output_bucket=args['S3_OUTPUT_BUCKET'],\r\n                s3_output_prefix= args['S3_OUTPUT_PREFIX'],\r\n                partition_keys=[\"year\", \"month\", \"day\", \"nom_setor\"]\r\n        )\r\n\r\n        create_table_if_not_exists(\r\n                database_name=args['DATABASE_NAME'],\r\n                table_name=args['TABLE_NAME'],\r\n                s3_output_bucket=args['S3_OUTPUT_BUCKET'],\r\n                s3_output_prefix= args['S3_OUTPUT_PREFIX'],\r\n                aws_region=args['AWS_REGION']\r\n        )\r\n\r\n        msck_repair_table(database_name=args['DATABASE_NAME'],table_name=args['TABLE_NAME'])\r\n        job.commit()\r\n        logger.info(\"Job Glue finalizado com sucesso.\")\r\n\r\nif __name__ == \"__main__\":\r\n    try:\r\n        main()\r\n    except Exception as e:\r\n        logger.error(f\"Falha na execução do job: {e}\")\r\n        raise"
}